{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from pandas import read_csv, set_option\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('display.max_rows', 25)\n",
    "\n",
    "set_option('display.max_columns', 25)\n",
    "\n",
    "set_option('display.max_colwidth', 25)\n",
    "\n",
    "set_option('display.width', 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "number_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f'Device: ', device, end='\\n\\n')\n",
    "\n",
    "print(f'Number of GPUs: ', number_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = './data/train.csv'\n",
    "\n",
    "TEST_DATA_PATH = './data/test.csv'\n",
    "\n",
    "TEST_DATA_LABELS_PATH = './data/test_label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = ['comment_text']\n",
    "\n",
    "LABEL_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overview: \\n\\n', train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null values: ', train_data.isnull().values.any())\n",
    "\n",
    "# train_data[train_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique sentences: ', train_data.comment_text.nunique() == train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average sentence length: ', train_data.comment_text.str.split().str.len().mean(), end='\\n\\n')\n",
    "\n",
    "print('Standard deviation sentence length: ', train_data.comment_text.str.split().str.len().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plots: \\n')\n",
    "\n",
    "train_data.hist(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label counts, may need to downsample or upsample\n",
    "print('Count of 1 per label: \\n\\n', train_data[LABEL_COLUMNS].sum(), '\\n')\n",
    "\n",
    "print('Count of 0 per label: \\n\\n', train_data[LABEL_COLUMNS].eq(0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffle rows\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One Hot Encode labels\n",
    "train_data['one_hot_labels'] = list(train_data[LABEL_COLUMNS].values)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select labels and sentences\n",
    "labels = list(train_data.one_hot_labels.values)\n",
    "\n",
    "number_labels = len(LABEL_COLUMNS)\n",
    "\n",
    "sentences = list(train_data.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform labels to tokens\n",
    "max_length = 10\n",
    "\n",
    "encodings = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # Tokenized and encoded sentences\n",
    "\n",
    "token_type_ids = encodings['token_type_ids'] # Token type ids\n",
    "\n",
    "attention_masks = encodings['attention_mask'] # Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying indices of One Hot Encode labels entries that only occur once. This makes\n",
    "# possible to stratify split the training data later, to take conclusions, i.e understand\n",
    "# the existing relationship between groups, which are made from each label encoded\n",
    "label_counts = train_data.one_hot_labels.astype(str).value_counts()\n",
    "\n",
    "one_frequency = label_counts[label_counts == 1].keys()\n",
    "\n",
    "one_frequency_idxs = sorted(\n",
    "    list(\n",
    "        train_data[train_data.one_hot_labels.astype(str).isin(one_frequency)].index\n",
    "    ),\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train label indices with only one instance: ', one_frequency_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_frequency_input_ids = [input_ids.pop(_id) for _id in one_frequency_idxs]\n",
    "\n",
    "one_frequency_token_types = [token_type_ids.pop(_id) for _id in one_frequency_idxs]\n",
    "\n",
    "one_frequency_attention_masks = [attention_masks.pop(_id) for _id in one_frequency_idxs]\n",
    "\n",
    "one_frequency_labels = [labels.pop(_id) for _id in one_frequency_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "(\n",
    "    train_inputs,\n",
    "    validation_inputs,\n",
    "    train_labels,\n",
    "    validation_labels,\n",
    "    train_token_types,\n",
    "    validation_token_types,\n",
    "    train_masks,\n",
    "    validation_masks\n",
    ") = train_test_split(\n",
    "    input_ids,\n",
    "    labels,\n",
    "    token_type_ids,\n",
    "    attention_masks,\n",
    "    random_state=10,\n",
    "    test_size=0.10,\n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_frequency_input_ids)\n",
    "\n",
    "train_labels.extend(one_frequency_labels)\n",
    "\n",
    "train_masks.extend(one_frequency_attention_masks)\n",
    "\n",
    "train_token_types.extend(one_frequency_token_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Convert all data into Torch Tensors, the required datatype for the model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "validation_token_types = torch.tensor(validation_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the\n",
    "# authors recommend a batch size of 32, 48, or 128. Use 32 to avoid\n",
    "# memory issues\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with Torch Data Loader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
    "\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(validation_dataloader,'validation_data_loader')\n",
    "\n",
    "# torch.save(train_dataloader,'train_data_loader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An alternative to sequence classification is multiple choise, which\n",
    "# learn to choose from varying options in contrast to sequence classification\n",
    "# which the choises (classes) do not vary across your samples, which\n",
    "# is exactly what\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=number_labels)\n",
    "\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom optimization parameters. You may implement a scheduler here as well.\n",
    "parameter_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "# Select parameters that are not in no decasy\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [\n",
    "            parameters for layer, parameters in parameter_optimizer if not any(_ in layer for _ in no_decay)\n",
    "        ],\n",
    "        'weight_decay_rate': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': [\n",
    "            parameters for layer, parameters in parameter_optimizer if any(_ in layer for _ in no_decay)\n",
    "        ],\n",
    "        'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=True)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(),lr=2e-5) # Default optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  #\n",
    "  # Training\n",
    "  #\n",
    "\n",
    "  # Set model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  training_loss = 0 #running loss\n",
    "    \n",
    "  training_examples, training_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(item.to(device) for item in batch)\n",
    "    \n",
    "    # Unpack the inputs from our data loader\n",
    "    batch_input_ids, batch_input_mask, batch_labels, batch_token_types = batch\n",
    "    \n",
    "    # Clear out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Forward pass for multi-class classification\n",
    "    # outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "    \n",
    "    # loss = outputs[0]\n",
    "    \n",
    "    # logits = outputs[1]\n",
    "\n",
    "    # Forward pass for multi-label classification\n",
    "    outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    \n",
    "    loss_function = BCEWithLogitsLoss() \n",
    "    \n",
    "    loss = loss_function(\n",
    "        logits.view(-1, number_labels),\n",
    "        batch_labels.type_as(logits).view(-1, number_labels)\n",
    "    ) # Convert labels to float for calculation\n",
    "    \n",
    "    # loss_function = BCELoss() \n",
    "    \n",
    "    # loss = loss_function(\n",
    "    #    torch.sigmoid(\n",
    "    #        logits.view(-1,number_labels)),\n",
    "    #        batch_labels.type_as(logits).view(-1,number_labels\n",
    "    #    )\n",
    "    # ) #convert labels to float for calculation\n",
    "    \n",
    "    train_loss_set.append(loss.item())    \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    training_loss += loss.item()\n",
    "    \n",
    "    training_examples += batch_input_ids.size(0)\n",
    "    \n",
    "    training_steps += 1\n",
    "\n",
    "  print('Train loss: {training_loss / training_steps:.4}')\n",
    "\n",
    "  #\n",
    "  # Validation\n",
    "  #\n",
    "    \n",
    "  # Set model to evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Variables to gather full output\n",
    "  logit_predictions, true_labels, predicted_labels, tokenized_sentences = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "  for step, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(item.to(device) for item in batch)\n",
    "    \n",
    "    # Unpack the inputs from our data loader\n",
    "    batch_input_ids, batch_input_mask, batch_labels, batch_token_types = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask)\n",
    "        \n",
    "      batch_logit_prediction = outputs[0]\n",
    "    \n",
    "      predicted_label = torch.sigmoid(batch_logit_prediction)\n",
    "\n",
    "      batch_logit_prediction = batch_logit_prediction.detach().cpu().numpy()\n",
    "    \n",
    "      predicted_label = predicted_label.to('cpu').numpy()\n",
    "        \n",
    "      batch_labels = batch_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_sentences.append(batch_input_ids)\n",
    "    \n",
    "    logit_predictions.append(batch_logit_prediction)\n",
    "    \n",
    "    true_labels.append(batch_labels)\n",
    "    \n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "  # Flatten outputs\n",
    "  predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "    \n",
    "  true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  threshold = 0.50\n",
    "\n",
    "  predicted_bools = [predicted_label > threshold for predicted_label in predicted_labels]\n",
    "    \n",
    "  true_bools = [true_label == 1 for true_label in true_labels]\n",
    "\n",
    "  value_f1_accuracy = f1_score(true_bools,predicted_bools,average='micro')*100\n",
    "    \n",
    "  value_flat_accuracy = accuracy_score(true_bools, predicted_bools)*100\n",
    "\n",
    "  print('F1 validation accuracy: ', value_f1_accuracy)\n",
    "    \n",
    "  print('Flat validation accuracy: ', value_flat_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
